{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end Neural Networks for NLP\n",
    "\n",
    "## Word Embeddings\n",
    "\n",
    "If everything is going well, you have used PyTorch to make a classifier based on logistic regression. Also, you are probably mildly annoyed by the fact that the accuracy was essentially the same as what we had with our previous version using scikit-learn, with the added difficulty that we had to write a loop all by ourselves.\n",
    "\n",
    "Now it is time to do some math and review our models.\n",
    "\n",
    "### Some underlying math of the bag-of-words model\n",
    "\n",
    "In our current model, we represent each word by a line-vector $x_n$ with $v$ dimensions, where $v$ is the length of our vocabulary. Therefore, a document with $n$ words can be represented by a matrix of words within a document $X^{(d)}$ as:\n",
    "\n",
    "$$\n",
    "X^{(d)} = \n",
    "\\begin{bmatrix} \n",
    "                        x^{(d)} _{1,1} & x^{(d)}_{1,2} & \\cdots & x^{(d)}_{1,d} \\\\\n",
    "                        x^{(d)}_{2,1} & x^{(d)}_{2,2} & \\cdots & x^{(d)}_{2,d} \\\\\n",
    "                        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                        x^{(d)}_{N,1} & x^{(d)}_{N,2} & \\cdots & x^{(d)}_{N,d} \\\\\n",
    "                        \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then, we essentially sum all elements of the matrix to get a count. This maps the sequence of words to a single vector $x$ that represents our document:\n",
    "\n",
    "$$\n",
    "x^\\prime_j = \\sum _{i=1}^n x^{(d)}_{i,j}\n",
    "$$\n",
    "\n",
    "This is the same as pre-multiplying $W$ by a line-matrix of ones:\n",
    "\n",
    "$$\n",
    "x^\\prime_j = [1, 1, \\cdots, 1] \\begin{bmatrix}\n",
    "x^{(d)}_1 \\\\\n",
    "x^{(d)}_2 \\\\ \n",
    "\\cdots \\\\\n",
    "x^{(d)}_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Last, if we want to binarize this vector, we simply apply an element-wise non-linearity such as:\n",
    "\n",
    "$$\n",
    "x = f(x^\\prime),\n",
    "$$\n",
    "where $f(x)=1$ if $x>0$ and $f(x)=0$ otherwise.\n",
    "\n",
    "Essentially, what we must do is:\n",
    "\n",
    "1. Get a representation for each word\n",
    "1. Combine the word representations for a document into a single representation\n",
    "1. Use some non-linearity to modulate the representation as needed\n",
    "\n",
    "Our solutions, so far, are:\n",
    "\n",
    "1. Use a one-hot encoding\n",
    "1. Use a simple summation\n",
    "1. Use a step-function with a suitable threshold\n",
    "\n",
    "The reason we have these solutions is that they somehow *make sense*. But now let's raise a question: are they the *best* solutions?\n",
    "\n",
    "### Problems with the sparse representation\n",
    "\n",
    "The one-hot encoding for words is easy to understand, but, at the same time, has some problems. \n",
    "\n",
    "The first, and more obvious, is that the dimensionality of the representation grows when the vocabulary grows. This is a problem because the amount of required data to train a system is typically a function of its dimension - hence, the more words we have, the more data we need. The vocabulary typically grows when we add more texts to the dataset, so this actually means: the more data we have, the more data we need.\n",
    "\n",
    "The second is that our data matrix starts getting too big. Maybe it stops fitting memory - and this is because we have so many columns! Maybe we could do something about this?\n",
    "\n",
    "The third, and less obvious, is that the distance between words is the same, regardless of their meaning. Hence, when we learn about dungeons, our system learns nothing about dragons. In fact, classifiers based on one-hot bag-of-words have no internal representation indicating that dungeons and dragons go more or less together.\n",
    "\n",
    "### Dense representations\n",
    "\n",
    "So, let's do something else. Let's say that each word is now going to be represented as a vector in some $\\mathbb{R}^N$ space. We are free to choose $N$. This representation is called *embedding*.\n",
    "\n",
    "Embeddings are *dense* representations, meaning that all dimensions are important. The name *dense* opposes to the *sparse* idea of the one-hot encoding in which only one dimension is important per word, and only a few dimensions are important per document.\n",
    "\n",
    "What we *want* here is to have all words being represented in locations of $\\mathbb{R}^N$ that reflect their meaning. But, what is *meaning*?\n",
    "\n",
    "This is a complicated question, which we will approach during this course.\n",
    "\n",
    "For now, let's go on and code a little.\n",
    "\n",
    "### Coding with embeddings\n",
    "\n",
    "To map words to their embeddings, we use a data structure that is very similar to a dictionary. The structure is wrapped in an *embedding layer* in Pytorch. \n",
    "\n",
    "It receives a sequence of integers as input. Each of these integer corresponds to a token in the vocabulary. For now, we will assume that each token corresponds to a word. Then, it yields a sequence of vectors for each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1735,  0.7323],\n",
      "         [ 0.8687,  1.9835],\n",
      "         [ 0.3441,  0.8275],\n",
      "         [ 1.1099, -0.6862]],\n",
      "\n",
      "        [[-1.5357, -1.0840],\n",
      "         [-0.0095, -0.4843],\n",
      "         [-1.5357, -1.0840],\n",
      "         [ 1.1099, -0.6862]],\n",
      "\n",
      "        [[-0.0095, -0.4843],\n",
      "         [-1.5357, -1.0840],\n",
      "         [ 1.1099, -0.6862],\n",
      "         [ 0.3441,  0.8275]]], grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([3, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Parameters of the embedding layer\n",
    "vocab_size = 100  # Number of unique tokens\n",
    "embedding_dim = 2  # You can choose the dimension of the embeddings\n",
    "embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "# Documents are now represented as sequences of tokens.\n",
    "# Each token is an integer representing the index of the token in the vocabulary.\n",
    "tokens = torch.tensor([[0, 1, 2, 3],\n",
    "                        [4, 5, 4, 3],\n",
    "                        [5, 4, 3, 2]])\n",
    "\n",
    "# Get the embeddings for the tokens\n",
    "embeddings = embedding_layer(tokens)\n",
    "\n",
    "print(embeddings)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, in the code above, we yielded a batch of three texts to the embedding layer. The batch has a length of four. Then, the embedding layer returns a tensor with tridimensional shape. The shape $3 \\times 4 \\times 2$ means:\n",
    "\n",
    "* We have 3 documents\n",
    "* Each document has 4 tokens\n",
    "* Each word is encoded in 2 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies for tokenization\n",
    "\n",
    "Just some minutes ago, we discussed that a token corresponds to a word. In fact, a token is a piece of the text that conveys some information - and this could be a word. Also, we have used n-grams as tokens when we wanted to consider the sequence of words.\n",
    "\n",
    "Now, let's remember. If we had around, maybe, $10^4$ words in our vocabulary, then we would have potentially $10^8$ bi-grams, and $10^{12}$ potential tri-grams. This is because we are aggreting more and more possible symbols.\n",
    "\n",
    "However, we could go on the opposite direction here.\n",
    "\n",
    "If we had the words \"working\", \"works\", \"walking\", \"walks\", \"calling\", and \"calls\", we could easily tokenize them into 6 tokens, each corresponding to full words.\n",
    "\n",
    "However, we could separate the suffixes and the stems. In this case, we could have tokens for: \"work\", \"walk\", \"call\", \"s\", and \"ing\". See: now we have 5 tokens instead of 6.\n",
    "\n",
    "This is generally called a \"subword\" tokenization strategy. One of the most used algorithms for such is called Sentence Piece Tokenizer. It works by building a vocabulary like this:\n",
    "\n",
    "1. Assume each character is a separate token\n",
    "1. Create a new token that merges the most common token sequence into a separate token\n",
    "1. Repeat the merging until we reach a reasonable vocabulary size\n",
    "\n",
    "A nice advantage of using this method is that it is less likely to find unknown tokens - afterall, we start from a limited vocabulary.\n",
    "\n",
    "Also, we are not going to build a tokenizer from scratch: instead, we are going to use a ready-made one. If you want to read all about it, refer to:\n",
    "\n",
    "[Kudo, T., and Richardson, J. SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (2018)](https://arxiv.org/pdf/1808.06226)\n",
    "\n",
    "### Example code for training a Sentence Piece tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: my_tokenizer\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 100\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 8 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=490\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=36\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 8 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=210\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 134 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 8\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 71\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 71 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=109 obj=13.1974 num_tokens=231 num_tokens/piece=2.11927\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=99 obj=12.7698 num_tokens=232 num_tokens/piece=2.34343\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: my_tokenizer.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: my_tokenizer.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "from io import StringIO\n",
    "\n",
    "# Your input data as a string\n",
    "input_data = \"\"\"Was ever feather so lightly blown to and fro as this multitude? The\n",
    "name of Henry the Fifth hales them to an hundred mischiefs and makes\n",
    "them leave me desolate. I see them lay their heads together to surprise\n",
    "me. My sword make way for me, for here is no staying.—In despite of the\n",
    "devils and hell, have through the very middest of you! And heavens and\n",
    "honour be witness that no want of resolution in me, but only my\n",
    "followers’ base and ignominious treasons, makes me betake me to my\n",
    "heels.\n",
    "\"\"\"\n",
    "\n",
    "# Use StringIO to create a file-like object\n",
    "input_fp = StringIO(input_data)\n",
    "\n",
    "# Train the SentencePiece model using the file pointer\n",
    "spm.SentencePieceTrainer.train(\n",
    "    sentence_iterator=input_fp, \n",
    "    model_prefix='my_tokenizer', \n",
    "    vocab_size=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example code to test the trained tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded pieces: ['▁', 'T', 'hi', 's', '▁', 'is', '▁', 'a', '▁', 't', 'es', 't', '▁s', 'e', 'n', 't', 'e', 'n', 'c', 'e', '.']\n",
      "Encoded ids: [3, 82, 90, 7, 3, 28, 3, 59, 3, 9, 54, 9, 30, 4, 14, 9, 4, 14, 83, 4, 20]\n",
      "Recovered sentence: This is a test sentence.\n"
     ]
    }
   ],
   "source": [
    "# Load the trained SentencePiece model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('my_tokenizer.model')\n",
    "\n",
    "# Test the tokenizer\n",
    "test_sentence = \"This is a test sentence.\"\n",
    "encoded_pieces = sp.encode_as_pieces(test_sentence)\n",
    "encoded_ids = sp.encode_as_ids(test_sentence)\n",
    "recovered_sentence = sp.decode_ids(encoded_ids)\n",
    "\n",
    "print(\"Encoded pieces:\", encoded_pieces)\n",
    "print(\"Encoded ids:\", encoded_ids)\n",
    "print(\"Recovered sentence:\", recovered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-padding and truncation: keeping sentences the same length\n",
    "\n",
    "If you have been paying attention, you probably realized that, in any language, sentences have different lengths. However, PyTorch works with tensors, hence the inputs for an embedding layer should have the same length.\n",
    "\n",
    "If we find long sentences, we could just truncate them so a desired length.\n",
    "\n",
    "However, if we find a short sentence, the procedure is different. Usually, we inser a special token called \"padding\" so that the sentence artificially becomes our desired length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: my_tokenizer\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 100\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: <PAD>\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 8 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <PAD>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=490\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=36\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 8 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=210\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 134 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 8\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 71\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 71 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=109 obj=13.1974 num_tokens=231 num_tokens/piece=2.11927\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=99 obj=12.7698 num_tokens=232 num_tokens/piece=2.34343\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: my_tokenizer.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: my_tokenizer.vocab\n"
     ]
    }
   ],
   "source": [
    "# Use StringIO to create a file-like object\n",
    "input_fp = StringIO(input_data)\n",
    "\n",
    "# Train the SentencePiece model using the file pointer\n",
    "spm.SentencePieceTrainer.train(\n",
    "    sentence_iterator=input_fp, \n",
    "    model_prefix='my_tokenizer', \n",
    "    vocab_size=100,\n",
    "    user_defined_symbols=['<PAD>']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('my_tokenizer.model')\n",
    "print(sp.piece_to_id('<PAD>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 83, 91, 8, 4, 29, 26, 6, 4, 10, 55, 10, 31, 5, 15, 10, 5, 15, 84, 5, 3, 3, 3, 3, 3], [4, 83, 91, 8, 4, 29, 78, 56, 10, 58, 4, 10, 55, 10, 31, 5, 15, 10, 5, 15, 84, 5, 3, 3, 3], [34, 29, 4, 29, 4, 60, 4, 35, 60, 96, 6, 40, 28, 42, 31, 5, 0, 27, 5, 15, 84, 5, 16, 4, 45]]\n",
      "torch.Size([3, 25, 2])\n"
     ]
    }
   ],
   "source": [
    "# Create an embedding layer with padding index set to 3\n",
    "def pad_to_len(sequences, pad_idx, max_len):\n",
    "    padded = []\n",
    "    for s in sequences:\n",
    "        if len(s) >= max_len:\n",
    "            padded.append(s[:max_len])\n",
    "        else:\n",
    "            padded.append(s + [pad_idx] * (max_len - len(s)))\n",
    "    return padded\n",
    "\n",
    "vocab_size = len(sp)\n",
    "embedding_dim = 2\n",
    "padding_idx = sp.piece_to_id('<PAD>')\n",
    "embedding_layer_with_padding = nn.Embedding(\n",
    "    num_embeddings=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    padding_idx=padding_idx,   \n",
    ")\n",
    "\n",
    "# Get the embeddings for the tokens using the new embedding layer\n",
    "input_data = [\"This is my test sentence\", \"This is another test sentence\", \"this is a really long sequence and I will probably have to crop it!\"]\n",
    "tokens = sp.tokenize(input_data)\n",
    "tokens = pad_to_len(tokens, padding_idx, 25)\n",
    "print(tokens)\n",
    "embeddings_with_padding = embedding_layer_with_padding(torch.tensor(tokens))\n",
    "#print(embeddings_with_padding)\n",
    "print(embeddings_with_padding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How far have we gone?\n",
    "\n",
    "So far, we have tokenized a batch of $b$ texts and have cropped/padded them to length $l$. After that, we calculated embeddings of dimension $d$ for our words. If everything went well, we now have embeddings $E \\in \\mathbb{R}^{b \\times l \\times d}$:\n",
    "\n",
    "```mermaid\n",
    "graph LR;\n",
    "    subgraph Tokenization;\n",
    "    Text([\"Texts (List of $$b$$ texts)\"]) --> A[\"Tokenizer\n",
    "        Cropping\n",
    "        Padding\"] --> B([\"Tokens (List, $$b \\times l$$)\"]) \n",
    "    end;\n",
    "\n",
    "    subgraph Embedding;\n",
    "    B --> C[\"Embedding\n",
    "    Layer\"] --> D([\"Embeddings ($$E \\in \\mathbb{R}^{b \\times l \\times d}$$)\"])\n",
    "    end;\n",
    "```\n",
    "\n",
    "However, if we are going to classify our text using Logistic Regression, we need a single vector to represent the whole sequence, that is, we need to convert:\n",
    "\n",
    "$$\n",
    "E \\in \\mathbb{R}^{b \\times l \\times d}\n",
    "$$\n",
    "\n",
    "to \n",
    "\n",
    "$$\n",
    "X \\in \\mathbb{R}^{b \\times d}\n",
    "$$\n",
    "\n",
    "In other words, we need to *summarize* the word-level embeddings into a single document-level embedding.\n",
    "\n",
    "The easiest way to do so is to calculate the mean of the embeddings, then we can proceed to classification:\n",
    "\n",
    "```mermaid\n",
    "graph LR;\n",
    "\n",
    "    subgraph Embedding;\n",
    "    D([\"Embeddings ($$E \\in \\mathbb{R}^{b \\times l \\times d}$$)\"])\n",
    "    end;\n",
    "\n",
    "    subgraph Summarization;\n",
    "    D --> E[\"Mean over time\"] --> F([\"Embeddings ($$X \\in \\mathbb{R}^{b \\times d}$$)\"])\n",
    "    end;\n",
    "\n",
    "    subgraph Classification;\n",
    "    F --> G[\"Logistic Regression\"] --> H([\"$$P(C_i=c_j | X_i)$$\"])\n",
    "    end;\n",
    "```\n",
    "\n",
    "Overall, we get 4-step architecture (tokenization, embedding, summarization, classification) that is, in fact, very similar to the one we had when we were using the vectorizer-classifier pipelines with the bag-of-words approach:\n",
    "\n",
    "```mermaid\n",
    "graph LR;\n",
    "\n",
    "    A([\"Text\"]) --> B[\"Tokenization\"];\n",
    "\n",
    "    subgraph Vectorization;\n",
    "    \n",
    "    B --> C[\"Embedding\"] --> D[\"Summarization\"];\n",
    "    end;\n",
    "    \n",
    "    subgraph Classification;\n",
    "    D --> E[\"Classification\"]\n",
    "    end;\n",
    " \n",
    "    E --> F([\"Prediction\"]);    \n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a pipeline with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that in Scikit-Learn we had pipelines? In PyTorch, the equivalent procedure is to make a class. This class should inherit from `nn.Module` and we have to define, at least, the methods:\n",
    "\n",
    "* `__init__`, which will initialize the class, instantiate all blocks that have parameters, and so on;\n",
    "* `forward`, which is called when the object is called (it bothers me that the more pythonic `__call__` was not chosen for this...). This method implements the actual workings of the pipeline. See this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleClassifier(\n",
      "  (embedding): Embedding(100, 2, padding_idx=3)\n",
      "  (clf): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "        self.clf = nn.Linear(embedding_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = self.clf(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleClassifier(vocab_size, embedding_dim)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/tiagoft/NLP/main/wiki_movie_plots_drama_comedy.csv'\n",
    ")\n",
    "# Split the data into training and testing sets\n",
    "texts = list(df['Plot'])\n",
    "labels = torch.tensor(list(df['Genre'].apply(lambda x: 1 if x == 'drama' else 0)))\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts,\n",
    "                                                    labels,\n",
    "                                                    test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/home/joaolucasmbc/mambaforge/envs/nlp/lib/python3.12/site-packages/torch/autograd/graph.py:823: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "100%|██████████| 200/200 [04:31<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "# We will also define an optimizer:\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e0) # lr is the learning rate - this is our alpha\n",
    "\n",
    "print(\"Entering loop\")\n",
    "# And now, this is the training loop:\n",
    "losses = []\n",
    "for epoch in tqdm(range(200)):\n",
    "    optimizer.zero_grad()\n",
    "    tokens = sp.encode_as_ids(X_train)\n",
    "    tokens = pad_to_len(tokens, padding_idx, 100)\n",
    "    tokens = torch.tensor(tokens)\n",
    "    output = model(tokens)\n",
    "    output_probs = torch.sigmoid(output)\n",
    "    loss = torch.mean( (output_probs-y_train)**2 )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model\n",
    "\n",
    "We can convert our "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5650072498791687\n",
      "F1 Score: 0.362048078846897\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.00      0.00       899\n",
      "           1       0.57      1.00      0.72      1170\n",
      "\n",
      "    accuracy                           0.57      2069\n",
      "   macro avg       0.45      0.50      0.36      2069\n",
      "weighted avg       0.46      0.57      0.41      2069\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Get the predictions for the test set\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    tokens = sp.encode_as_ids(X_test)\n",
    "    tokens = pad_to_len(tokens, padding_idx, 100)\n",
    "    tokens = torch.tensor(tokens)\n",
    "    output = model(tokens)\n",
    "    output_probs = torch.sigmoid(output)\n",
    "    predictions = (output_probs > 0.5).int().numpy()\n",
    "\n",
    "# Calculate the accuracy and F1 score\n",
    "accuracy = accuracy_score(\n",
    "    y_test,\n",
    "    predictions,\n",
    ")\n",
    "f1 = f1_score(\n",
    "    y_test,\n",
    "    predictions,\n",
    "    zero_division=0,\n",
    "    average='macro',\n",
    ")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    predictions,\n",
    "    zero_division=0,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some steps on optimization\n",
    "\n",
    "You may have noticed that we are using the MSE loss in the code above. This is definitely not optimal, because reductions in MSE+ does not necessarily correspond to increases in accuracy. Instead, we could use the cross-entropy loss.\n",
    "\n",
    "### Cross-entropy loss\n",
    "\n",
    "The idea of the cross-entropy loss begins with the negative log likelihood (NLL). Is is usually written as:\n",
    "\n",
    "$$\n",
    "\\text{NLL} = - \\sum_i \\log{P(C_i = c_i|X_i)},\n",
    "$$\n",
    "\n",
    "where $C_i=c_i$ is the event that the predicted class $C_i$ for the $i$-th input $X_i$ is the correct class $c_i$.\n",
    "\n",
    "Note that NLL measures the likelihoood that the correct class will be predicted for each item $i$ in the dataset. Therefore, maximizing the negative log likelihood means minimizing the probability that a wrong class will be predicted. Importantly, this is *very* different from minimizing the MSE!\n",
    "\n",
    "We know that $P(C_i = c_i|X_i)$, in the logistic regressor, is the result of applying the sigmoid (logistic) function to the logits $z_i$. I will call $y_i = P(C_i = c_i|X_i)$ to make reading easier:\n",
    "\n",
    "$$\n",
    "y_i = \\sigma(z_i) = \\frac{1}{1+e^{-z_i}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "We can substitute $y_i$ in the NLL equation to get:\n",
    "\n",
    "$$\n",
    "\\text{NLL} = - \\sum_i \\log{y_i} = -\\sum_i \\log{\\frac{1}{1+e^{-z_i}}},\n",
    "$$\n",
    "\n",
    "Remember that the log of a division is the subtraction of logs. Hence, we get:\n",
    "\n",
    "$$\n",
    "\\text{NLL} = -\\sum_i \\log{1} - \\log{1+e^{-z_i}} = -\\sum_i 0 - \\log{1+e^{-z_i}} = \\sum_i \\log{(1+e^{-z_i})}.\n",
    "$$\n",
    "\n",
    "Also, we are using SGD as the optimizer. The SGD optimizer uses a fixed-step optimization over the parameter space. A more modern approach is the Adam (Adaptive Momentum) optimizer, which adapts the step size so that smoother regions of the parameter space are swept more quickly, and rougher regions automatically lead to smaller steps.\n",
    "\n",
    "Then, the code becomes this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: my_tokenizer\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 10000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: <PAD>\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(380) LOG(WARNING) Found too long line (4293 > 4192).\n",
      "trainer_interface.cc(382) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(383) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 21657 sentences\n",
      "trainer_interface.cc(416) LOG(INFO) Skipped 61 too long sentences.\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <PAD>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=15965242\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.955% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=74\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.99955\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 21657 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=7958278\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 187723 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 21657\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 152783\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 152783 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=71031 obj=11.0702 num_tokens=324567 num_tokens/piece=4.56937\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=61013 obj=8.80151 num_tokens=326345 num_tokens/piece=5.34878\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=45754 obj=8.77836 num_tokens=344319 num_tokens/piece=7.52544\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=45740 obj=8.76346 num_tokens=344359 num_tokens/piece=7.52862\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=34305 obj=8.8551 num_tokens=373294 num_tokens/piece=10.8816\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=34304 obj=8.83452 num_tokens=373300 num_tokens/piece=10.8821\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=25728 obj=8.96854 num_tokens=406114 num_tokens/piece=15.7849\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=25728 obj=8.94286 num_tokens=406109 num_tokens/piece=15.7847\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=19296 obj=9.12156 num_tokens=439780 num_tokens/piece=22.7913\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=19296 obj=9.08956 num_tokens=439776 num_tokens/piece=22.791\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=14472 obj=9.30044 num_tokens=472529 num_tokens/piece=32.6513\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=14472 obj=9.26324 num_tokens=472497 num_tokens/piece=32.649\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=11000 obj=9.49926 num_tokens=502705 num_tokens/piece=45.7005\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=11000 obj=9.4595 num_tokens=502691 num_tokens/piece=45.6992\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: my_tokenizer.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: my_tokenizer.vocab\n"
     ]
    }
   ],
   "source": [
    "input_fp = StringIO(' '.join(X_train))\n",
    "spm.SentencePieceTrainer.train(\n",
    "    sentence_iterator=input_fp, \n",
    "    model_prefix='my_tokenizer', \n",
    "    vocab_size=10000,\n",
    "    user_defined_symbols=['<PAD>']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('my_tokenizer.model')\n",
    "print(sp.piece_to_id('<PAD>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleClassifier(\n",
      "  (embedding): Embedding(10000, 2, padding_idx=3)\n",
      "  (clf): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "        self.clf = nn.Linear(embedding_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = self.clf(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleClassifier(10000, embedding_dim)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [07:11<00:00,  2.16s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# We will also define an optimizer:\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=1e0)  # lr is the learning rate - this is our alpha\n",
    "\n",
    "print(\"Entering loop\")\n",
    "# And now, this is the training loop:\n",
    "losses = []\n",
    "model.train()\n",
    "for epoch in tqdm(range(200)):\n",
    "    optimizer.zero_grad()\n",
    "    tokens = sp.encode_as_ids(X_train)\n",
    "    tokens = pad_to_len(tokens, padding_idx, 1000)\n",
    "    tokens = torch.tensor(tokens)\n",
    "    output = model(tokens)\n",
    "    loss = torch.mean(\n",
    "        torch.binary_cross_entropy_with_logits(\n",
    "            output.flatten().float(),\n",
    "            y_train.flatten().float(),\n",
    "        ), )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7462542290961818\n",
      "F1 Score: 0.7403498145176539\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.69      0.70       899\n",
      "           1       0.77      0.79      0.78      1170\n",
      "\n",
      "    accuracy                           0.75      2069\n",
      "   macro avg       0.74      0.74      0.74      2069\n",
      "weighted avg       0.75      0.75      0.75      2069\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Get the predictions for the test set\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    tokens = sp.encode_as_ids(X_test)\n",
    "    tokens = pad_to_len(tokens, padding_idx, 1000)\n",
    "    tokens = torch.tensor(tokens)\n",
    "    output = model(tokens)\n",
    "    output_probs = torch.sigmoid(output)\n",
    "    predictions = (output_probs > 0.5).int().numpy()\n",
    "\n",
    "# Calculate the accuracy and F1 score\n",
    "accuracy = accuracy_score(\n",
    "    y_test,\n",
    "    predictions,\n",
    ")\n",
    "f1 = f1_score(\n",
    "    y_test,\n",
    "    predictions,\n",
    "    zero_division=0,\n",
    "    average='macro',\n",
    ")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    predictions,\n",
    "    zero_division=0,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### 1 - Visualizing embeddings\n",
    "\n",
    "Change the code for the `SimpleClassifier` class by adding a method that returns the word-level embeddings. Then, calculate the embedding for each word in your vocabulary. Plot the embeddings before training the model, and then after training the model. Do you see a pattern there?\n",
    "\n",
    "### 2 - Further optimization\n",
    "\n",
    "Review the whole code for the classifier. Identify all parameters you can change. Identify the possible performance bottlenecks, change the parameters to reduce these bottlenecks, and then run the training process again. Do you see changes?\n",
    "\n",
    "### 3 - Advanced modelling\n",
    "\n",
    "Change the `forward` method in `SimpleClassifier` so that the mean only accounts for non-PAD tokens (that is: PAD tokens are ignored). Does that improve performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
