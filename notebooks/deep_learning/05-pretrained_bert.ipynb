{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c4e4871",
   "metadata": {},
   "source": [
    "# BERT\n",
    "\n",
    "**GOAL: At the end of this class, we will be able to USE a pre-trained BERT to (a) generate suggestions and to (b) generate embeddings for classification**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e01af4e",
   "metadata": {},
   "source": [
    "## What is BERT?\n",
    "\n",
    "After the [transformer](https://arxiv.org/abs/1706.03762), we had many other advances. One of such, of course, is the [GPT](https://paperswithcode.com/paper/improving-language-understanding-by), which uses a decoder-only transformer architecture to predict the next word in a sentence. GPT uses a decoder-only architecture because it needs the masked multi-head attention device to avoid making trivial predictions. Ultimately, GPT generates an embedding space that increases the likelihood of choosing meaningful words for a text continuation.\n",
    "\n",
    "The Google team found another interesting way to obtain this type of representation. They trained an *encoder*-only transformer that can predict words removed from the text - similarly to how we know what is missing in  \"Luke, I am your ____\". The idea here is that we can use information from the future for this task, because it is highly dependent on context. Simultaneously, they trained the model to classify whether two given phrases follow each other in a corpus. So, BERT was born."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c00a497",
   "metadata": {},
   "source": [
    "\n",
    "```mermaid\n",
    "graph LR;\n",
    "    subgraph Input;\n",
    "    T[\"Token embeddings\"];\n",
    "    P[\"Position embeddings\"];\n",
    "    S[\"Segment embeddings \n",
    "    (indicates if it is sentence 1\n",
    "     or sentence 2 in NSP task)\"];\n",
    "    ADD([\"\\+\"]);\n",
    "    T --> ADD;\n",
    "    P --> ADD;\n",
    "    S --> ADD; \n",
    "    end;\n",
    "\n",
    "    SEQ[\"Sequence Model\"];\n",
    "    ADD --> SEQ;\n",
    "    RES[\"Result: 1 vector per input token\"];\n",
    "    SEQ --> RES;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366ef601",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Bert stands for [Bidirectional Encoder Representations from Transformers, and was introduced in this paper from 2019](https://arxiv.org/pdf/1810.04805). The greatest contribution of BERT, besides its architecture, is the idea of training the language model for different tasks at the same time.\n",
    "\n",
    "We are definitely not going to train BERT in class, but we are using it for other tasks. We will use the [BERT implementation from Hugging Face](https://huggingface.co/google-bert/bert-base-uncased). All help files are here.\n",
    "\n",
    "## Task 1: Masked Language Model\n",
    "\n",
    "The first task BERT was trained for was the Masked Language Model. This was inspired in a task called [\"Cloze\"](https://en.wikipedia.org/wiki/Cloze_test), and the idea is to remove a word from a sentence and let the system predict what word should fill that sentence:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211077b9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```mermaid\n",
    "graph LR;\n",
    "    subgraph Inputs;\n",
    "    INPUT[\"[CLS]\n",
    "        remove\n",
    "        some\n",
    "        parts\n",
    "        [MASK]\n",
    "        a\n",
    "        sentence\"];\n",
    "    end;\n",
    "    INPUT --> BERT[\"BERT\"];\n",
    "    subgraph Outputs;\n",
    "    OUTPUT[\"C\n",
    "    T1\n",
    "    T2\n",
    "    T3\n",
    "    T4\n",
    "    T5\n",
    "    T6\"];\n",
    "    end;\n",
    "    BERT --> OUTPUT;\n",
    "    Train[\"Loss: T4 should be the word 'of'\"]\n",
    "    OUTPUT --- Train;\n",
    "```\n",
    "\n",
    "\n",
    "This task suggests that the embedding space created by BERT should allow representing words in the context of the rest of the sentence!\n",
    "\n",
    "To play with this task with Hugging Face's library, you can use:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48f66818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9431135058403015,\n",
       "  'token': 1997,\n",
       "  'token_str': 'of',\n",
       "  'sequence': 'remove some parts of a sentence.'},\n",
       " {'score': 0.049855172634124756,\n",
       "  'token': 2013,\n",
       "  'token_str': 'from',\n",
       "  'sequence': 'remove some parts from a sentence.'},\n",
       " {'score': 0.00420895591378212,\n",
       "  'token': 1999,\n",
       "  'token_str': 'in',\n",
       "  'sequence': 'remove some parts in a sentence.'},\n",
       " {'score': 0.0006226626574061811,\n",
       "  'token': 2306,\n",
       "  'token_str': 'within',\n",
       "  'sequence': 'remove some parts within a sentence.'},\n",
       " {'score': 0.0005233768024481833,\n",
       "  'token': 2076,\n",
       "  'token_str': 'during',\n",
       "  'sequence': 'remove some parts during a sentence.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "unmasker(\"Remove some parts [MASK] a sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bf1f3f",
   "metadata": {},
   "source": [
    "### Algorithmic bias and Hallucinations\n",
    "\n",
    "Note that BERT is generating words that make sense. However, these continuations do not necessarily correspond to reality. In fact, these continuations are simply something that maximizes a probability related to a specific dataset!\n",
    "\n",
    "Check, for example, the output for:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dbe9a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.07573239505290985,\n",
       "  'token': 4511,\n",
       "  'token_str': 'wine',\n",
       "  'sequence': 'kentucky is famous for its wine.'},\n",
       " {'score': 0.06742823123931885,\n",
       "  'token': 14746,\n",
       "  'token_str': 'wines',\n",
       "  'sequence': 'kentucky is famous for its wines.'},\n",
       " {'score': 0.02818020060658455,\n",
       "  'token': 12212,\n",
       "  'token_str': 'beaches',\n",
       "  'sequence': 'kentucky is famous for its beaches.'},\n",
       " {'score': 0.022783879190683365,\n",
       "  'token': 12846,\n",
       "  'token_str': 'cuisine',\n",
       "  'sequence': 'kentucky is famous for its cuisine.'},\n",
       " {'score': 0.02119813859462738,\n",
       "  'token': 5194,\n",
       "  'token_str': 'horses',\n",
       "  'sequence': 'kentucky is famous for its horses.'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "unmasker(\"Kentucky is famous for its [MASK].\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55f1899",
   "metadata": {},
   "source": [
    "\n",
    "Kentucky is a state in the USA that may or may not have wineries, but definitely does not have famous beaches! Now, check the output when you change Kentucky for the Brazilian state of Minas Gerais!\n",
    "\n",
    "See - there is no \"brain\" inside BERT. There is merely a system that finds plausible completions for a task. This is something we have been calling \"hallucinations\" in LLMs. In the end, the model is just as biased as the dataset used for training it.\n",
    "\n",
    "### Algorithmic prejudice\n",
    "\n",
    "Despite the funny things things that the model could output, there are some assertions that can be dangerous, or outright sexist. Try to see the output of:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77017a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.0693001076579094,\n",
       "  'token': 3460,\n",
       "  'token_str': 'doctor',\n",
       "  'sequence': 'a successful man works as a doctor.'},\n",
       " {'score': 0.06541889905929565,\n",
       "  'token': 5160,\n",
       "  'token_str': 'lawyer',\n",
       "  'sequence': 'a successful man works as a lawyer.'},\n",
       " {'score': 0.0410967618227005,\n",
       "  'token': 7500,\n",
       "  'token_str': 'farmer',\n",
       "  'sequence': 'a successful man works as a farmer.'},\n",
       " {'score': 0.03909463807940483,\n",
       "  'token': 10533,\n",
       "  'token_str': 'carpenter',\n",
       "  'sequence': 'a successful man works as a carpenter.'},\n",
       " {'score': 0.0385400615632534,\n",
       "  'token': 22701,\n",
       "  'token_str': 'tailor',\n",
       "  'sequence': 'a successful man works as a tailor.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "unmasker(\"A successful man works as a [MASK].\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0ce1077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.15128055214881897,\n",
       "  'token': 19215,\n",
       "  'token_str': 'prostitute',\n",
       "  'sequence': 'a successful woman works as a prostitute.'},\n",
       " {'score': 0.08569061756134033,\n",
       "  'token': 10850,\n",
       "  'token_str': 'maid',\n",
       "  'sequence': 'a successful woman works as a maid.'},\n",
       " {'score': 0.05853833258152008,\n",
       "  'token': 6821,\n",
       "  'token_str': 'nurse',\n",
       "  'sequence': 'a successful woman works as a nurse.'},\n",
       " {'score': 0.050308708101511,\n",
       "  'token': 3460,\n",
       "  'token_str': 'doctor',\n",
       "  'sequence': 'a successful woman works as a doctor.'},\n",
       " {'score': 0.04774889722466469,\n",
       "  'token': 13877,\n",
       "  'token_str': 'waitress',\n",
       "  'sequence': 'a successful woman works as a waitress.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "unmasker(\"A successful woman works as a [MASK].\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af373066",
   "metadata": {},
   "source": [
    "\n",
    "Now, change \"man\" for \"woman\". The result is not as pretty. But, see, this is not a problem of the language model structure per se - rather, it is a problem of the data used to train it.\n",
    "\n",
    "We could go on finding examples of other types of prejudice - there are all sorts of sexism and racism lying in the hidden spaces of BERT.\n",
    "\n",
    "This is bad, but remember this was 2019, and people were impressed that the system could generate coherent words at all! Nowadays, LLM outputs go through a filter that finds phrases that are potentially harmful, so they don't write ugly phrases.\n",
    "\n",
    "Which of the phrases below are true about this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa293a1",
   "metadata": {},
   "source": [
    "## Task 2: Next Sentence Prediction\n",
    "\n",
    "BERT was also trained for a task called Next Sentence Prediction. The idea of this task is to insert two sentences in the input of BERT, separating them with a special [SEP] token. Then, the system uses the output of the [CLS] token to classify whether these two sentences do or do not follow each other. It is something like:\n",
    "\n",
    "```mermaid\n",
    "graph LR;\n",
    "    subgraph Inputs;\n",
    "    INPUT[\"[CLS]\n",
    "        Here\n",
    "        I\n",
    "        am\n",
    "        [SEP]\n",
    "        rock\n",
    "        you\n",
    "        like\n",
    "        a\n",
    "        hurricane\"];\n",
    "    end;\n",
    "    INPUT --> BERT[\"BERT\"];\n",
    "    subgraph Outputs;\n",
    "    OUTPUT[\"C\n",
    "    T1\n",
    "    T2\n",
    "    etc\"];\n",
    "    end;\n",
    "    BERT --> OUTPUT;\n",
    "    Train[\"Loss: C should be equal to 1\"]\n",
    "    OUTPUT --- Train;\n",
    "```\n",
    "\n",
    "```mermaid\n",
    "graph LR;\n",
    "    subgraph Inputs;\n",
    "    INPUT[\"[CLS]\n",
    "        Here\n",
    "        I\n",
    "        am\n",
    "        [MASK]\n",
    "        rock\n",
    "        your\n",
    "        body\"];\n",
    "    end;\n",
    "    INPUT --> BERT[\"BERT\"];\n",
    "    subgraph Outputs;\n",
    "    OUTPUT[\"C\n",
    "    T1\n",
    "    T2\n",
    "    etc\"];\n",
    "    end;\n",
    "    BERT --> OUTPUT;\n",
    "    Train[\"Loss: C should be equal to 0\"]\n",
    "    OUTPUT --- Train;\n",
    "```\n",
    "\n",
    "The consequence of this training is that the embedding $C$ of the [CLS] token represents the content of the rest of the tokens. Hence, we can use it for classification. For such, we can go straight to the HuggingFace library and use:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1fb9a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1dac23",
   "metadata": {},
   "source": [
    "\n",
    "The embedding for the [CLS] token can be accessed using:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "750e786e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.3863e-01,  1.5827e-01, -2.9666e-01, -1.8494e-01, -4.6429e-01,\n",
      "        -5.0329e-01, -6.2349e-04,  1.0469e+00,  1.7754e-01, -3.9585e-02,\n",
      "        -2.5098e-01, -1.4966e-01,  7.2868e-02,  1.9662e-01,  1.2086e-01,\n",
      "         8.5213e-02, -5.2902e-01,  4.4599e-01,  3.2803e-02,  1.2729e-01,\n",
      "        -1.8406e-01, -1.9723e-01, -1.4142e-02, -4.3873e-02,  3.0995e-01,\n",
      "         3.6228e-01,  3.2393e-02,  5.5499e-02, -3.6191e-02, -1.9490e-01,\n",
      "         1.9163e-01,  4.5594e-01, -4.3608e-01, -3.3814e-01,  3.3220e-01,\n",
      "         1.3876e-01,  3.7784e-01,  7.7509e-02,  7.1754e-02, -1.7745e-01,\n",
      "        -8.0696e-01, -2.4013e-01,  4.4476e-01,  5.7503e-01,  4.5728e-01,\n",
      "        -7.4665e-01, -2.6650e+00, -1.1582e-01, -2.4457e-01, -2.5926e-01,\n",
      "         8.1343e-02, -1.4255e-01,  5.7927e-01,  4.3881e-01,  3.4623e-01,\n",
      "         2.2726e-01, -5.3047e-01, -2.9977e-03,  1.3115e-01,  2.7001e-01,\n",
      "        -4.0535e-01,  2.5096e-01, -3.9678e-01, -3.0917e-02, -1.0593e-01,\n",
      "         5.9698e-01, -1.3360e-01,  8.6452e-01, -8.6112e-01,  2.9506e-01,\n",
      "        -5.0009e-01, -4.7448e-01,  2.4661e-01, -4.1733e-01, -1.6709e-01,\n",
      "         7.7702e-02, -3.0343e-01,  8.7029e-02, -6.0070e-01,  3.3935e-02,\n",
      "        -2.3271e-02,  8.2941e-01,  5.9883e-02, -6.3124e-01,  3.8525e-01,\n",
      "         2.3305e-01, -9.7491e-01, -6.2403e-01,  3.0751e-02,  7.6039e-01,\n",
      "        -4.4270e-01, -2.8010e-01,  4.1858e-01,  7.1768e-01,  3.6274e-01,\n",
      "         4.4406e-01,  3.6096e-01, -1.8213e-01, -4.6562e-01,  1.3700e-01,\n",
      "        -3.2337e-04, -2.9695e-01,  3.4885e-01, -6.8974e-01,  2.5790e-01,\n",
      "         1.5364e-01, -3.6245e-01, -6.4651e-01,  1.0972e-01, -1.8250e+00,\n",
      "         6.0310e-01,  8.5845e-02, -4.8385e-01, -5.6315e-02, -4.8095e-01,\n",
      "         2.6047e-01,  6.9181e-01, -1.2619e-01,  9.1778e-02, -6.9652e-02,\n",
      "         2.7353e-01,  1.1629e-01, -1.8121e-01, -3.2614e-01, -3.2568e-03,\n",
      "         3.0560e-02,  2.5153e-01, -2.1205e-01,  4.0272e-01, -7.1996e-02,\n",
      "         5.1724e-01,  1.1586e+00,  3.0930e-01, -2.9087e-01, -3.9949e-01,\n",
      "         2.5565e-01,  7.2059e-01,  2.2208e-01,  2.9908e-01, -5.6428e-01,\n",
      "        -3.5746e-01, -5.9386e-01, -2.6760e+00, -1.1018e-01,  1.2430e+00,\n",
      "         4.2583e-01,  4.0041e-01, -1.3441e-01,  2.2572e-01, -1.7879e-02,\n",
      "        -2.5958e-01,  2.1460e-01, -2.8132e-01,  3.8727e-02, -3.6085e-01,\n",
      "         6.9585e-02, -2.7510e-01, -5.7470e-01,  7.1369e-01,  1.2321e-01,\n",
      "         6.9163e-01, -7.6787e-01, -6.6482e-03, -6.9368e-02, -1.8612e-01,\n",
      "         3.8029e-01,  4.7945e-01,  1.3711e-01,  1.5347e-01,  5.7105e-01,\n",
      "        -5.1635e-01,  4.9148e-01,  1.3239e+00,  2.5844e-01,  5.8631e-02,\n",
      "        -3.9467e-02, -2.5362e-01,  6.4861e-01,  7.2386e-02, -1.8788e-01,\n",
      "        -4.9633e-01,  3.3056e-01,  2.6669e-01, -1.7058e-01,  2.8774e-01,\n",
      "        -1.1491e-01,  6.2969e-01, -8.4275e-02, -6.3055e-01, -1.6193e-01,\n",
      "        -1.6229e-01, -1.2086e-01,  2.9440e-01,  5.8662e-01,  5.4674e-01,\n",
      "        -5.3363e-01,  1.7337e-01, -2.9141e-01,  1.2820e-01,  1.0300e+00,\n",
      "         4.0335e-01, -2.6932e-01, -3.1945e-01, -7.0103e-02, -3.7277e-01,\n",
      "         3.7754e+00, -2.4594e-01, -3.4773e-01, -3.4763e-01,  3.8996e-01,\n",
      "        -8.4587e-01,  1.2379e-01, -1.4505e-01, -1.6805e-01,  1.2478e-01,\n",
      "         1.6549e-01, -2.7173e-01,  1.6656e-01, -2.8498e-01, -1.5413e-02,\n",
      "        -2.1442e-01,  2.9599e-01, -6.2169e-01,  5.0246e-01, -2.1452e-01,\n",
      "        -1.4969e-01, -6.4084e-01,  9.8417e-01, -1.4672e-01, -1.2301e+00,\n",
      "         4.4040e-01,  2.2934e-01,  2.7561e-01,  7.7660e-01, -4.0361e-01,\n",
      "        -1.4733e-01, -1.1069e+00, -3.0339e-01, -2.8682e-01, -4.4347e-02,\n",
      "         7.5044e-02, -1.6130e-01,  2.6377e-01,  4.7523e-01, -7.8318e-01,\n",
      "         9.5241e-01,  2.5885e-01, -1.1017e-01,  9.0016e-01, -1.2896e-01,\n",
      "         5.8264e-01, -2.9905e-01,  5.2767e-01, -8.2960e-01,  6.6971e-01,\n",
      "         3.6185e-01, -3.4812e-01,  3.5801e-01, -5.5639e-01, -4.9350e-01,\n",
      "        -4.3962e-01,  3.9874e-02,  3.2327e-01, -4.6462e-01, -6.8221e-01,\n",
      "        -7.9491e-01,  4.4669e-01,  2.3415e-02, -4.5427e-01, -5.7032e-01,\n",
      "         1.0452e-01, -3.6552e-01, -6.1703e-01, -3.1135e+00, -1.7814e-01,\n",
      "         8.5957e-03, -1.7504e-01,  6.8890e-01, -9.6681e-02,  2.5503e-01,\n",
      "         2.6883e-01,  5.2414e-01, -1.2743e+00,  1.0247e+00, -1.2809e-01,\n",
      "        -2.1350e-01,  3.5362e-01, -1.0532e-01, -1.0826e-01,  1.9288e-01,\n",
      "        -3.9588e-01, -7.2014e-02, -3.6901e-01, -2.2861e-01,  5.6277e-02,\n",
      "        -5.1787e-01,  3.9186e-01,  3.2096e-01, -1.7476e-02, -2.5212e-01,\n",
      "        -1.5539e-02, -3.1079e-01, -5.0350e-01,  2.8213e-01, -8.0737e-03,\n",
      "         4.6251e-01, -4.7324e-02, -7.6220e-01, -1.2954e+00, -9.5769e-02,\n",
      "        -1.6045e-01, -4.2052e-01, -3.4707e-01, -7.5433e-02,  3.3622e-01,\n",
      "         1.2618e-01, -1.1184e+00,  2.4624e-01,  8.3222e-02, -5.0087e-02,\n",
      "        -3.0261e-01,  2.7831e-01,  4.7121e-01,  5.3416e-01,  5.9849e-01,\n",
      "        -2.1806e-01,  1.0635e-01,  7.1929e-01, -3.2277e-01,  4.9508e-01,\n",
      "         2.3190e-01,  2.5173e-01,  2.4508e-01,  1.0428e+00, -4.9004e-01,\n",
      "        -3.9466e-01, -1.6659e-01, -2.1985e-01,  1.7801e-01, -4.8173e-01,\n",
      "         9.7710e-02,  2.0431e-01, -3.6752e-01, -2.4564e-01,  2.2330e-01,\n",
      "         3.0553e-01,  5.3448e-01, -1.4259e-01, -6.7158e-02,  8.9229e-01,\n",
      "        -2.6886e-01,  1.2369e-01,  1.0365e+00,  5.0009e-01,  2.6082e-01,\n",
      "         1.5436e-01,  4.8452e-01,  3.5387e-01, -7.1846e-02,  3.0719e-01,\n",
      "         9.3322e-01,  1.3701e-01,  2.4093e-01,  2.4618e-01,  2.2994e-01,\n",
      "        -3.3645e-01,  2.7115e-01,  9.5728e-02,  9.1890e-01, -5.3342e-01,\n",
      "         5.2959e-01, -1.6737e-01,  2.8627e-01, -7.1014e-01, -2.6933e-01,\n",
      "        -5.9314e-01,  7.5477e-02,  3.0465e-01,  1.5903e-02,  3.0582e-01,\n",
      "        -2.0786e-01, -9.6327e-01, -5.2609e-02,  5.9653e-01, -3.2123e-01,\n",
      "        -1.0026e-01,  1.1383e-01, -2.9222e-01, -3.4897e-01, -2.0320e-01,\n",
      "        -1.0693e+00,  7.2272e-01, -4.0494e-01, -4.6370e-01,  1.7093e-01,\n",
      "        -6.5278e-02, -7.6814e-01, -6.1509e-01, -1.9261e-01,  5.9381e-01,\n",
      "         1.5674e-01,  6.6132e-01, -3.2267e-01,  8.9210e-03,  7.3028e-01,\n",
      "        -1.0432e+00,  3.8613e-02, -1.6959e-01, -2.1638e-01, -5.3767e-01,\n",
      "         2.7999e-01, -5.7204e-02, -2.6547e-01,  1.1488e-01, -4.8089e-01,\n",
      "        -5.2453e-01,  4.2675e-02,  2.1987e-01, -6.4553e-01, -5.8105e-01,\n",
      "        -1.1154e-01,  2.4987e-01,  3.3998e-01, -1.3087e-01, -5.8519e-02,\n",
      "         7.7398e-01, -1.8964e-02,  4.8722e-01,  1.5242e-01,  3.4393e-01,\n",
      "        -3.5194e-01, -1.4068e-01, -6.1876e-01, -4.3536e-01,  1.5986e-01,\n",
      "        -8.2145e-01,  5.5696e-02, -7.0877e-01,  4.2778e-01, -1.9793e-01,\n",
      "        -7.7728e-01,  1.0092e-01,  6.6808e-01, -6.5057e-01, -4.3033e-01,\n",
      "        -2.9843e-01, -1.5036e-01,  2.4834e-01, -1.5298e-01,  3.0706e-02,\n",
      "        -5.6209e-01,  1.5665e-01, -4.3619e-01,  8.1211e-01, -3.3395e-01,\n",
      "        -6.7091e-02, -2.7624e-01,  5.1148e-01, -5.1986e-01, -2.0125e-01,\n",
      "        -4.8436e-01, -8.3396e-01,  1.3664e-01, -1.0073e-01, -5.2947e-02,\n",
      "        -9.0816e-02,  5.4192e-01, -1.5238e-01, -4.5153e-01,  9.2602e-02,\n",
      "        -1.5649e+00,  5.0334e-01,  6.2166e-01,  2.0573e-01,  6.6985e-01,\n",
      "        -1.8345e-01, -7.5642e-01,  1.1160e+00,  1.0200e-01,  2.3684e-01,\n",
      "        -2.7487e-01, -5.6173e-01,  2.0236e-01,  6.7757e-01,  6.9223e-02,\n",
      "         1.3245e-01,  3.3535e-01, -4.4464e-01, -5.7931e-01, -9.4346e-01,\n",
      "        -5.0326e-01,  5.8444e-01,  5.3691e-02, -4.1365e-01, -4.3387e-01,\n",
      "        -5.4087e-02,  6.7094e-02,  5.1723e-02, -3.1266e-01,  5.1134e-01,\n",
      "        -2.1455e-01, -4.1089e-01, -7.6648e-01, -3.7814e-01,  2.2427e-01,\n",
      "         1.5365e-01,  6.8970e-01, -2.7090e-01,  1.2699e+00,  4.4815e-01,\n",
      "        -1.2705e-01,  7.0326e-01,  2.8316e-01,  2.0543e-01,  5.0365e-01,\n",
      "         2.3280e-01, -8.5071e-01,  4.6223e-01,  4.4362e-02, -7.4848e-01,\n",
      "        -4.7291e-01, -5.7542e-01,  2.8405e-01, -5.8890e-02,  4.3108e-01,\n",
      "        -4.1656e-01,  6.1470e-01, -4.5749e-02, -8.9509e-01, -9.8707e-02,\n",
      "         2.7836e-01, -1.6658e-01, -5.8064e-01,  2.8151e-01, -2.6772e-01,\n",
      "        -3.8511e-01,  6.9710e-03, -7.8095e-01, -8.7858e-01,  1.0314e-01,\n",
      "         4.5656e-01,  2.9397e-01, -2.3487e-01,  6.2479e-01, -6.9897e-01,\n",
      "        -9.7354e-02, -8.2375e-02,  3.2827e-02,  4.5677e-01, -1.5334e-01,\n",
      "         4.5654e-01, -8.3556e-01, -6.5947e-01,  5.6639e-02, -3.0414e-01,\n",
      "         9.2307e-01, -5.2615e-01,  1.7381e-02,  1.6742e-01, -8.1372e-01,\n",
      "        -5.8711e-01, -6.1384e-01,  3.8924e-01, -1.1150e-01, -4.4166e-01,\n",
      "        -2.6542e-01,  1.6358e-01,  7.5758e-01,  5.5392e-01,  4.4207e-01,\n",
      "         1.6300e-01,  8.5946e-01,  6.5089e-01,  1.8756e-01,  4.3742e-01,\n",
      "        -3.9318e-01,  5.8097e-01, -6.1632e-01, -1.9909e-01, -2.2360e-02,\n",
      "         1.1466e-01, -2.8252e-01, -5.4943e-01, -1.4298e-02,  3.6538e-01,\n",
      "        -2.6172e-01,  1.2320e-01, -2.4256e-01,  1.8361e+00,  6.9208e-01,\n",
      "        -3.6687e-01, -4.3950e-01,  5.3324e-01, -1.7550e-01, -5.3212e-01,\n",
      "         2.6075e-01, -7.1782e-01,  5.3808e-01, -6.8672e-01,  6.3705e-01,\n",
      "        -3.5609e-01,  3.4474e-01,  6.2619e-01,  2.8950e-01, -1.4197e-02,\n",
      "        -3.1484e-01, -6.5851e-01,  2.9190e-01, -1.9953e-01,  1.0749e+00,\n",
      "         7.3925e-01, -8.0275e-02,  2.7612e-01,  9.7417e-01, -5.5230e-02,\n",
      "        -3.3466e-01,  4.7410e-01,  5.4011e-01, -6.4445e-01,  5.6686e-02,\n",
      "         2.5149e-01,  6.7781e-01, -8.1888e-01,  6.6071e-01, -1.5622e-01,\n",
      "        -1.1766e-01, -1.9790e-01,  7.7745e-02,  7.8141e-02, -2.4193e-01,\n",
      "         1.1015e+00,  1.6805e-01, -8.2389e-02,  9.1502e-01, -2.6633e-01,\n",
      "        -9.8394e-01,  8.6338e-01,  4.7132e-01,  2.1046e-01, -1.2781e-01,\n",
      "        -9.5984e-02, -3.1228e-02, -3.0741e-01, -4.1697e-01,  3.6279e-01,\n",
      "         4.8002e-01, -8.2228e-01,  3.8157e-01,  1.1428e-01,  3.9590e-01,\n",
      "         8.7067e-02, -5.3801e-01,  1.4278e-01,  1.4168e-01, -3.4284e-01,\n",
      "         1.1206e-01,  9.3911e-02, -3.8531e-01,  3.4362e-01,  3.4614e-01,\n",
      "         2.0893e-01,  5.5559e-01,  3.3607e-01,  4.0920e-01,  5.0197e-01,\n",
      "         2.5993e-01, -6.8478e-01, -2.3775e+00,  5.1888e-01,  3.5783e-01,\n",
      "        -4.0208e-02, -2.5484e-01,  7.3479e-02,  2.4950e-01,  4.3718e-01,\n",
      "         6.4128e-01,  8.5277e-02,  6.9360e-01,  2.2754e-01,  1.1564e+00,\n",
      "        -9.4982e-02, -9.3017e-02,  5.3614e-01,  2.5788e-01, -5.0486e-01,\n",
      "         1.4877e-01, -7.5893e-01,  4.2096e-01,  2.5936e-01, -4.0181e-01,\n",
      "        -4.2079e-01, -2.9521e-01,  8.2347e-02, -2.5238e-01, -7.7289e-01,\n",
      "        -3.6355e-02,  1.2557e+00, -5.9894e-01,  6.7479e-01, -4.8016e-01,\n",
      "         2.7968e-01,  1.7516e-01, -4.5409e-01, -1.3632e-01,  7.2885e-01,\n",
      "         9.6521e-01,  1.9018e-01, -4.4518e-01,  4.1336e-01, -4.7443e-01,\n",
      "         3.0883e-01,  9.2767e-02,  3.7638e-01,  1.0424e+00,  1.2319e-02,\n",
      "         6.0464e-01, -2.3269e-01, -2.3642e-01, -1.7452e-01,  6.3593e-01,\n",
      "         4.1798e-02, -1.5246e-01, -1.8308e-01,  2.5567e-01,  1.2592e-01,\n",
      "        -3.9873e-01, -6.5010e-01,  3.7672e-01,  6.1102e-01, -4.1082e-01,\n",
      "         2.4426e-01,  3.4739e-01, -3.5330e-01,  2.9130e-01, -1.3485e-02,\n",
      "        -3.3615e-01, -4.9522e-01, -4.3792e-01, -5.0794e-01,  2.8735e-01,\n",
      "         1.7634e-01, -2.6147e-02,  6.3740e-02,  3.3569e-01, -1.1070e-01,\n",
      "         5.1132e-01,  9.2538e-02, -5.8465e-01, -5.0504e-01,  6.8190e-02,\n",
      "         2.7989e-01, -1.3576e-01, -6.9536e+00, -1.5772e-01, -8.6210e-02,\n",
      "        -1.5084e-01, -6.6476e-01, -2.1086e-01,  2.3124e-01, -4.9271e-01,\n",
      "         2.7153e-02,  1.4571e-01,  5.8892e-02, -2.5410e-01, -1.4256e-01,\n",
      "        -2.7085e-01, -2.8436e-01,  4.5808e-01], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output_cls = output.last_hidden_state[0,0,:]\n",
    "print(output_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94a9cb9",
   "metadata": {},
   "source": [
    "    \n",
    "There are many details in this implementation, so I made a [video exploring them all](https://youtu.be/FXtGq_TYLzM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfd4017",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Our usual way to approach classification is to do something in the lines of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee984b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      comedy       0.76      0.17      0.28        76\n",
      "       drama       0.66      0.97      0.78       124\n",
      "\n",
      "    accuracy                           0.67       200\n",
      "   macro avg       0.71      0.57      0.53       200\n",
      "weighted avg       0.70      0.67      0.59       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/tiagoft/NLP/main/wiki_movie_plots_drama_comedy.csv').sample(1000)\n",
    "X = df['Plot']\n",
    "y = df['Genre']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline with TfidfVectorizer and LogisticRegression\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('logreg', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Test the pipeline\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5876be",
   "metadata": {},
   "source": [
    "Now, instead of using a TfIdf vectorizer, calculate embeddings for the texts in the dataset using BERT. Then, use *them* to classify. Compare the results with the ones we have when we use the Bag-of-words approach.\n",
    "\n",
    "Justify these results using the concept of embeddings we have studied in the previous lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24000a2c",
   "metadata": {},
   "source": [
    "## **Por que next sentence prediction implica na habilidade de classificar um texto em classes?**\n",
    "\n",
    "LER BERT ARTIGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f634d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "# Make your solution here\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/tiagoft/NLP/main/wiki_movie_plots_drama_comedy.csv').sample(1000)\n",
    "X = df['Plot']\n",
    "y = df['Genre']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "X_embeddings = []\n",
    "for text in tqdm(X):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    output = model(**encoded_input)\n",
    "    X_embeddings.append(output.last_hidden_state[0, 0, :].detach().numpy())\n",
    "\n",
    "X_embeddings = np.array(X_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6c94287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      comedy       0.72      0.60      0.65        85\n",
      "       drama       0.74      0.83      0.78       115\n",
      "\n",
      "    accuracy                           0.73       200\n",
      "   macro avg       0.73      0.71      0.72       200\n",
      "weighted avg       0.73      0.73      0.73       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_embeddings, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline with TfidfVectorizer and LogisticRegression\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the pipeline\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Test the pipeline\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bb02d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:56<00:00,  4.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# This is my solution - do not copy it!\n",
    "\n",
    "# Step 0: get data\n",
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/tiagoft/NLP/main/wiki_movie_plots_drama_comedy.csv').sample(1000)\n",
    "X = df['Plot']\n",
    "y = df['Genre']\n",
    "\n",
    "# Step 1: preprocess the text\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "def get_embeddings(text, model, tokenizer):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    cls_embedding = outputs.last_hidden_state[0, 0, :]\n",
    "    return cls_embedding\n",
    "\n",
    "embeddings = []\n",
    "for i in tqdm(range(len(X))):\n",
    "    e = get_embeddings(X.iloc[i], model, tokenizer)\n",
    "    embeddings.append(e.detach().numpy())\n",
    "embeddings = np.array(embeddings)\n",
    "np.save('bert_embeddings.npy', embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b709b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      comedy       0.69      0.77      0.72        77\n",
      "       drama       0.84      0.78      0.81       123\n",
      "\n",
      "    accuracy                           0.78       200\n",
      "   macro avg       0.76      0.77      0.77       200\n",
      "weighted avg       0.78      0.78      0.78       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings = np.load('bert_embeddings.npy')\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, y, test_size=0.2, random_state=42)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afa1ade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdd94d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
